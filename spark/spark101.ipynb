{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.) Create a spark data frame that contains your favorite programming languages.\n",
    "\n",
    "The name of the column should be language\n",
    "\n",
    "View the schema of the dataframe\n",
    "\n",
    "Output the shape of the dataframe\n",
    "\n",
    "Show the first 5 records in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>experience_level</th>\n",
       "      <th>teachers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Python</td>\n",
       "      <td>2</td>\n",
       "      <td>Ryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ruby</td>\n",
       "      <td>1</td>\n",
       "      <td>Zach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HTML</td>\n",
       "      <td>3</td>\n",
       "      <td>Self</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Java</td>\n",
       "      <td>1</td>\n",
       "      <td>Ryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C++</td>\n",
       "      <td>0</td>\n",
       "      <td>Zach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RedLion</td>\n",
       "      <td>3</td>\n",
       "      <td>Self</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Honeywell</td>\n",
       "      <td>3</td>\n",
       "      <td>Randy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    language  experience_level teachers\n",
       "0     Python                 2     Ryan\n",
       "1       Ruby                 1     Zach\n",
       "2       HTML                 3     Self\n",
       "3       Java                 1     Ryan\n",
       "4        C++                 0     Zach\n",
       "5    RedLion                 3     Self\n",
       "6  Honeywell                 3    Randy"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "table_data = {\"language\" : [\"Python\", \"Ruby\", \"HTML\", \"Java\", \"C++\", \"RedLion\", \"Honeywell\"],\n",
    "             \"experience_level\" : [2, 1, 3, 1, 0, 3, 3],\n",
    "             \"teachers\" : [\"Ryan\", \"Zach\", \"Self\", \"Ryan\", \"Zach\", \"Self\", \"Randy\"]}\n",
    "\n",
    "pandas_df = pd.DataFrame(table_data, columns=[\"language\", \"experience_level\", \"teachers\"])\n",
    "\n",
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[language: string, experience_level: bigint, teachers: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turning it into a Spark dataframe:\n",
    "\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- experience_level: long (nullable = true)\n",
      " |-- teachers: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the schema (structure)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 rows 3 columns\n"
     ]
    }
   ],
   "source": [
    "# Show the shape\n",
    "\n",
    "df_shape = df.count(), len(df.columns)\n",
    "\n",
    "print(df.count(), \"rows\", len(df.columns), \"columns\")\n",
    "\n",
    "\n",
    "\n",
    "# def spark_shape(self):\n",
    "#     return (self.count(), len(self.columns))\n",
    "# pyspark.sql.dataframe.DataFrame.shape = spark_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+--------+\n",
      "|language|experience_level|teachers|\n",
      "+--------+----------------+--------+\n",
      "|  Python|               2|    Ryan|\n",
      "|    Ruby|               1|    Zach|\n",
      "|    HTML|               3|    Self|\n",
      "|    Java|               1|    Ryan|\n",
      "|     C++|               0|    Zach|\n",
      "+--------+----------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, language: string, experience_level: string, teachers: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------------------+--------+\n",
      "|summary|language|  experience_level|teachers|\n",
      "+-------+--------+------------------+--------+\n",
      "|  count|       7|                 7|       7|\n",
      "|   mean|    null|1.8571428571428572|    null|\n",
      "| stddev|    null|1.2149857925879117|    null|\n",
      "|    min|     C++|                 0|   Randy|\n",
      "|    max|    Ruby|                 3|    Zach|\n",
      "+-------+--------+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.) Load the mpg dataset as a spark dataframe.\n",
    "\n",
    "Create 1 column of output that contains a message like the one below:\n",
    "\n",
    "\n",
    "The 1999 audi a4 has a 4 cylinder engine.\n",
    "For each vehicle.\n",
    "\n",
    "Transform the trans column so that it only contains either manual or auto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----+----+---+----------+---+---+---+---+-------+\n",
      "|manufacturer|     model|displ|year|cyl|     trans|drv|cty|hwy| fl|  class|\n",
      "+------------+----------+-----+----+---+----------+---+---+---+---+-------+\n",
      "|        audi|        a4|  1.8|1999|  4|  auto(l5)|  f| 18| 29|  p|compact|\n",
      "|        audi|        a4|  1.8|1999|  4|manual(m5)|  f| 21| 29|  p|compact|\n",
      "|        audi|        a4|  2.0|2008|  4|manual(m6)|  f| 20| 31|  p|compact|\n",
      "|        audi|        a4|  2.0|2008|  4|  auto(av)|  f| 21| 30|  p|compact|\n",
      "|        audi|        a4|  2.8|1999|  6|  auto(l5)|  f| 16| 26|  p|compact|\n",
      "|        audi|        a4|  2.8|1999|  6|manual(m5)|  f| 18| 26|  p|compact|\n",
      "|        audi|        a4|  3.1|2008|  6|  auto(av)|  f| 18| 27|  p|compact|\n",
      "|        audi|a4 quattro|  1.8|1999|  4|manual(m5)|  4| 18| 26|  p|compact|\n",
      "|        audi|a4 quattro|  1.8|1999|  4|  auto(l5)|  4| 16| 25|  p|compact|\n",
      "|        audi|a4 quattro|  2.0|2008|  4|manual(m6)|  4| 20| 28|  p|compact|\n",
      "+------------+----------+-----+----+---+----------+---+---+---+---+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading mpg\n",
    "\n",
    "from pydataset import data\n",
    "\n",
    "mpg = spark.createDataFrame(data(\"mpg\"))\n",
    "\n",
    "mpg.show(10)\n",
    "# from pydataset import data\n",
    "\n",
    "# mpg = spark.createDataFrame(data(\"mpg\"))\n",
    "# mpg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- manufacturer: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- displ: double (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- cyl: long (nullable = true)\n",
      " |-- trans: string (nullable = true)\n",
      " |-- drv: string (nullable = true)\n",
      " |-- cty: long (nullable = true)\n",
      " |-- hwy: long (nullable = true)\n",
      " |-- fl: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[manufacturer: string, model: string, displ: double, year: bigint, cyl: bigint, trans: string, drv: string, cty: bigint, hwy: bigint, fl: string, class: string, details: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# creating a column of output with that reading:\n",
    "\n",
    "from pyspark.sql.functions import col, expr, lit, when\n",
    "\n",
    "\n",
    "mpg = mpg.withColumn('details', lit('these are the details'))\n",
    "\n",
    "display(mpg)\n",
    "\n",
    "# # The 1999 audi a4 has a 4 cylinder engine. For each vehicle.\n",
    "\n",
    "# from pyspark.sql.functions import col, expr, lit, when\n",
    "\n",
    "# def car_description(year, model, car):\n",
    "    \n",
    "#     date = mpg.select(mpg.year)\n",
    "#     maker = mpg.select(mpg.manufacturer)\n",
    "#     style = mpg.select(mpg.model)\n",
    "#     pistons = mpg.select(mpg.cyl)\n",
    "\n",
    "#     return f\"The {date} {maker} {style} has a {pistons} engine.\"\n",
    "\n",
    "# car_description(\"2008\", \"a4 quattro\", \"audi\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-----------------+------------------+-----------------+------------------+----------+---+------------------+-----------------+----+-------+--------------------+\n",
      "|summary|manufacturer|            model|             displ|             year|               cyl|     trans|drv|               cty|              hwy|  fl|  class|             details|\n",
      "+-------+------------+-----------------+------------------+-----------------+------------------+----------+---+------------------+-----------------+----+-------+--------------------+\n",
      "|  count|         234|              234|               234|              234|               234|       234|234|               234|              234| 234|    234|                 234|\n",
      "|   mean|        null|             null| 3.471794871794873|           2003.5| 5.888888888888889|      null|4.0|16.858974358974358|23.44017094017094|null|   null|                null|\n",
      "| stddev|        null|             null|1.2919590310839348|4.509646313320409|1.6115344846842894|      null|0.0| 4.255945678889395|5.954643441166446|null|   null|                null|\n",
      "|    min|        audi|      4runner 4wd|               1.6|             1999|                 4|  auto(av)|  4|                 9|               12|   c|2seater|these are the det...|\n",
      "|    max|  volkswagen|toyota tacoma 4wd|               7.0|             2008|                 8|manual(m6)|  r|                35|               44|   r|    suv|these are the det...|\n",
      "+-------+------------+-----------------+------------------+-----------------+------------------+----------+---+------------------+-----------------+----+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**^^ Need Help with this one.  Spent WAYYYY too long on it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [manufacturer#403, model#404, displ#405, year#406L, cyl#407L, trans#408, drv#409, cty#410L, hwy#411L, fl#412, class#413, these are the details AS details#459]\n",
      "+- Scan ExistingRDD[manufacturer#403,model#404,displ#405,year#406L,cyl#407L,trans#408,drv#409,cty#410L,hwy#411L,fl#412,class#413]\n"
     ]
    }
   ],
   "source": [
    "# transform the columns\n",
    "\n",
    "mpg.explain() # '.explain()' shows us how spark is thinking about our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [trans#408]\n",
      "+- Scan ExistingRDD[manufacturer#403,model#404,displ#405,year#406L,cyl#407L,trans#408,drv#409,cty#410L,hwy#411L,fl#412,class#413]\n"
     ]
    }
   ],
   "source": [
    "mpg.select(mpg.trans).explain() # shows us how Spark is thinking about our 'trans' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maggie's Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.) Create a spark data frame that contains your favorite programming languages.\n",
    "\n",
    "The name of the column should be language\n",
    "\n",
    "View the schema of the dataframe\n",
    "\n",
    "Output the shape of the dataframe\n",
    "\n",
    "Show the first 5 records in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[language: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe of languages w/ one (1) column named 'language'\n",
    "\n",
    "pd_df = pd.DataFrame({\"language\" : [\"r\", \"python\", \"sql\", \"english\", \"spanish\", \"french\", \"julia\", \"pig latin\", \"lorem ipsum\"]})\n",
    "\n",
    "df = spark.createDataFrame(pd_df)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- language: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View the structure, or \"schema\"\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Shape:  9  rows X  1  column\n"
     ]
    }
   ],
   "source": [
    "# View the shape (her way)\n",
    "\n",
    "print(\"DataFrame Shape: \", df.count(), \" rows X \" , len(df.columns), \" column\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|language|\n",
      "+--------+\n",
      "|       r|\n",
      "|  python|\n",
      "|     sql|\n",
      "| english|\n",
      "| spanish|\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The first 5 records:\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.) Load the mpg dataset as a spark dataframe.\n",
    "\n",
    "Create 1 column of output that contains a message like the one below:\n",
    "\n",
    "\n",
    "The 1999 audi a4 has a 4 cylinder engine.\n",
    "For each vehicle.\n",
    "\n",
    "Transform the trans column so that it only contains either manual or auto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------+\n",
      "|vehicle_cylinder_desc                                         |\n",
      "+--------------------------------------------------------------+\n",
      "|The 1999 audi a4 has a 4 cylinder engine.                     |\n",
      "|The 1999 audi a4 has a 4 cylinder engine.                     |\n",
      "|The 2008 audi a4 has a 4 cylinder engine.                     |\n",
      "|The 2008 audi a4 has a 4 cylinder engine.                     |\n",
      "|The 1999 audi a4 has a 6 cylinder engine.                     |\n",
      "|The 1999 audi a4 has a 6 cylinder engine.                     |\n",
      "|The 2008 audi a4 has a 6 cylinder engine.                     |\n",
      "|The 1999 audi a4 quattro has a 4 cylinder engine.             |\n",
      "|The 1999 audi a4 quattro has a 4 cylinder engine.             |\n",
      "|The 2008 audi a4 quattro has a 4 cylinder engine.             |\n",
      "|The 2008 audi a4 quattro has a 4 cylinder engine.             |\n",
      "|The 1999 audi a4 quattro has a 6 cylinder engine.             |\n",
      "|The 1999 audi a4 quattro has a 6 cylinder engine.             |\n",
      "|The 2008 audi a4 quattro has a 6 cylinder engine.             |\n",
      "|The 2008 audi a4 quattro has a 6 cylinder engine.             |\n",
      "|The 1999 audi a6 quattro has a 6 cylinder engine.             |\n",
      "|The 2008 audi a6 quattro has a 6 cylinder engine.             |\n",
      "|The 2008 audi a6 quattro has a 8 cylinder engine.             |\n",
      "|The 2008 chevrolet c1500 suburban 2wd has a 8 cylinder engine.|\n",
      "|The 2008 chevrolet c1500 suburban 2wd has a 8 cylinder engine.|\n",
      "+--------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create 1 column of output that contains a message like the one below:\n",
    "# \"The 1999 audi a4 has a 4 cylinder engine. For each vehicle.\"\n",
    "\n",
    "# * NB: the 'mpg' data was loaded from pydataset in the cells above, with \n",
    "# mpg = spark.createDataFrame(pydataset.data('mpg'))\n",
    "\n",
    "mpg.select(concat(\n",
    "    lit('The '),\n",
    "    col('year'), \n",
    "    lit(' '), \n",
    "    col('manufacturer'), \n",
    "    lit(' '), \n",
    "    col('model'), \n",
    "    lit(' has a '), \n",
    "    col('cyl'), \n",
    "    lit(' cylinder engine.')).alias('vehicle_cylinder_desc')).show(truncate = False)\n",
    "\n",
    "# so instead of f-strings, Spark seems to use 'col.'  If you want the data from that \n",
    "# column, just 'col('column_name').'  Whatever words go between the column data are \n",
    "# represented with 'lit('words_I_want_to_use').'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+----------+\n",
      "|trans_extract|trans_replace|trans_when|\n",
      "+-------------+-------------+----------+\n",
      "|         auto|         auto|      auto|\n",
      "|       manual|       manual|    manual|\n",
      "|       manual|       manual|    manual|\n",
      "|         auto|         auto|      auto|\n",
      "|         auto|         auto|      auto|\n",
      "|       manual|       manual|    manual|\n",
      "|         auto|         auto|      auto|\n",
      "|       manual|       manual|    manual|\n",
      "|         auto|         auto|      auto|\n",
      "|       manual|       manual|    manual|\n",
      "|         auto|         auto|      auto|\n",
      "|         auto|         auto|      auto|\n",
      "|       manual|       manual|    manual|\n",
      "|         auto|         auto|      auto|\n",
      "|       manual|       manual|    manual|\n",
      "|         auto|         auto|      auto|\n",
      "|         auto|         auto|      auto|\n",
      "|         auto|         auto|      auto|\n",
      "|         auto|         auto|      auto|\n",
      "|         auto|         auto|      auto|\n",
      "+-------------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transforming 'trans' column to only show either 'manual' or 'auto.'  \n",
    "# Apparently, there are a lot of ways to do this, but Maggie's going with \n",
    "# regex extract, replace, and when\n",
    "\n",
    "mpg.select(\n",
    "    regexp_extract('trans', r'^(\\w+)\\(', 1).alias('trans_extract'),\n",
    "    # she's regexing from the start of the string (^) and all the letters in the\n",
    "    # capture group (\\w+) and ASK HERE\n",
    "    regexp_replace('trans', r'\\(.+$', '').alias('trans_replace'),\n",
    "    when(mpg.trans.like('auto%'), 'auto').otherwise('manual').alias('trans_when')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.) Load the tips dataset as a spark dataframe.\n",
    "\n",
    "- What percentage of observations are smokers?\n",
    "\n",
    "- Create a column that contains the tip percentage\n",
    "\n",
    "- Calculate the average tip percentage for each combination of sex and smoker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "\n",
    "from pydataset import data\n",
    "\n",
    "tips = spark.createDataFrame(data(\"tips\"))\n",
    "\n",
    "tips.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+\n",
      "|smoker|count|percent|\n",
      "+------+-----+-------+\n",
      "|    No|  151|    62%|\n",
      "|   Yes|   93|    38%|\n",
      "+------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# % of total are smokers?\n",
    "\n",
    "tips.groupBy(\"smoker\").count().withColumn(\"percent\", \n",
    "                        concat(round((col('count')/tips.count()*100), 0).cast(\"int\"),\n",
    "                        lit(\"%\"))).show()\n",
    "\n",
    "# group 'tips' by smoker count, add column \"percent,\" round the math that makes \n",
    "# \"percent,\" make the data in that column an integer (.cast(\"int\")), and add\n",
    "# the literal '%' sign to the number (lit(\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|smoker|count|\n",
      "+------+-----+\n",
      "|    No|  151|\n",
      "|   Yes|   93|\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+-------+\n",
      "|smoker|count|percent|\n",
      "+------+-----+-------+\n",
      "|    No|  151|   62.0|\n",
      "|   Yes|   93|   38.0|\n",
      "+------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smoker_prop = tips.groupBy(\"smoker\").count()\n",
    "smoker_prop.show()\n",
    "\n",
    "smoker_prop.withColumn(\"percent\", round((col(\"count\") / tips.count() * 100), 0)).show()\n",
    "\n",
    "# \"percent\" is the new column, and all the stuff after \"percent\" is the stuff that's in\n",
    "# that new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+------------------+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|       tip_percent|\n",
      "+----------+----+------+------+---+------+----+------------------+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|5.9446733372572105|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|16.054158607350097|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|16.658733936220845|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2| 13.97804054054054|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|14.680764538430255|\n",
      "|     25.29|4.71|  Male|    No|Sun|Dinner|   4| 18.62396204033215|\n",
      "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2| 22.80501710376283|\n",
      "|     26.88|3.12|  Male|    No|Sun|Dinner|   4|11.607142857142858|\n",
      "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|13.031914893617023|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|21.853856562922868|\n",
      "|     10.27|1.71|  Male|    No|Sun|Dinner|   2| 16.65043816942551|\n",
      "|     35.26| 5.0|Female|    No|Sun|Dinner|   4|14.180374361883155|\n",
      "|     15.42|1.57|  Male|    No|Sun|Dinner|   2|10.181582360570687|\n",
      "|     18.43| 3.0|  Male|    No|Sun|Dinner|   4|16.277807921866522|\n",
      "|     14.83|3.02|Female|    No|Sun|Dinner|   2|20.364126770060686|\n",
      "|     21.58|3.92|  Male|    No|Sun|Dinner|   2|18.164967562557923|\n",
      "|     10.33|1.67|Female|    No|Sun|Dinner|   3| 16.16650532429816|\n",
      "|     16.29|3.71|  Male|    No|Sun|Dinner|   3|22.774708410067525|\n",
      "|     16.97| 3.5|Female|    No|Sun|Dinner|   3|20.624631703005306|\n",
      "|     20.65|3.35|  Male|    No|Sat|Dinner|   3|16.222760290556902|\n",
      "+----------+----+------+------+---+------+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips.withColumn(\"tip_percent\", col(\"tip\") / col(\"total_bill\") * 100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`tip_percent`' given input columns: [day, size, tip, sex, time, total_bill, smoker];;\\n'Aggregate [sex#1259, smoker#1260], [sex#1259, smoker#1260, avg('tip_percent) AS avg_tip_pct#1516]\\n+- LogicalRDD [total_bill#1257, tip#1258, sex#1259, smoker#1260, day#1261, time#1262, size#1263L], false\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o418.agg.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`tip_percent`' given input columns: [day, size, tip, sex, time, total_bill, smoker];;\n'Aggregate [sex#1259, smoker#1260], [sex#1259, smoker#1260, avg('tip_percent) AS avg_tip_pct#1516]\n+- LogicalRDD [total_bill#1257, tip#1258, sex#1259, smoker#1260, day#1261, time#1262, size#1263L], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:65)\n\tat org.apache.spark.sql.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:224)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-157c1fbc10d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtips\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sex\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"smoker\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tip_percent\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"avg_tip_pct\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/group.py\u001b[0m in \u001b[0;36magg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexprs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"all exprs should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             jdf = self._jgd.agg(exprs[0]._jc,\n\u001b[0;32m--> 115\u001b[0;31m                                 _to_seq(self.sql_ctx._sc, [c._jc for c in exprs[1:]]))\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`tip_percent`' given input columns: [day, size, tip, sex, time, total_bill, smoker];;\\n'Aggregate [sex#1259, smoker#1260], [sex#1259, smoker#1260, avg('tip_percent) AS avg_tip_pct#1516]\\n+- LogicalRDD [total_bill#1257, tip#1258, sex#1259, smoker#1260, day#1261, time#1262, size#1263L], false\\n\""
     ]
    }
   ],
   "source": [
    "tips.groupBy(\"sex\", \"smoker\").agg(mean(col(\"tip_percent\")).alias(\"avg_tip_pct\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.) Use the seattle weather dataset referenced in the lesson to answer the questions below.\n",
    "\n",
    "- Convert the temperatures to farenheight.\n",
    "\n",
    "- Which month has the most rain, on average?\n",
    "\n",
    "- Which year was the windiest?\n",
    "\n",
    "- What is the most frequent type of weather in January?\n",
    "\n",
    "- What is the average high and low tempurature on sunny days in July in 2013 and 2014?\n",
    "\n",
    "- What percentage of days were rainy in q3 of 2015?\n",
    "\n",
    "- For each year, find what percentage of days it rained (had non-zero precipitation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+--------+--------+----+-------+\n",
      "|               date|precipitation|temp_max|temp_min|wind|weather|\n",
      "+-------------------+-------------+--------+--------+----+-------+\n",
      "|2012-01-01 00:00:00|          0.0|    12.8|     5.0| 4.7|drizzle|\n",
      "|2012-01-02 00:00:00|         10.9|    10.6|     2.8| 4.5|   rain|\n",
      "|2012-01-03 00:00:00|          0.8|    11.7|     7.2| 2.3|   rain|\n",
      "|2012-01-04 00:00:00|         20.3|    12.2|     5.6| 4.7|   rain|\n",
      "+-------------------+-------------+--------+--------+----+-------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the dataset\n",
    "\n",
    "import vega_datasets\n",
    "\n",
    "from vega_datasets import data\n",
    "\n",
    "weather = data.seattle_weather()\n",
    "\n",
    "weather = spark.createDataFrame(weather)\n",
    "\n",
    "weather.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+--------+--------+----+-------+\n",
      "|               date|precipitation|temp_max|temp_min|wind|weather|\n",
      "+-------------------+-------------+--------+--------+----+-------+\n",
      "|2012-01-01 00:00:00|          0.0|   55.04|    41.0| 4.7|drizzle|\n",
      "|2012-01-02 00:00:00|         10.9|   51.08|   37.04| 4.5|   rain|\n",
      "|2012-01-03 00:00:00|          0.8|   53.06|   44.96| 2.3|   rain|\n",
      "|2012-01-04 00:00:00|         20.3|   53.96|   42.08| 4.7|   rain|\n",
      "|2012-01-05 00:00:00|          1.3|   48.02|   37.04| 6.1|   rain|\n",
      "+-------------------+-------------+--------+--------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert temps from C to F:\n",
    "\n",
    "weather = (weather.withColumn(\"temp_max\", (col(\"temp_max\") * 9/5 +32))\n",
    "          .withColumn(\"temp_min\", (col(\"temp_min\") * 9/5 + 32)))\n",
    "\n",
    "# add column (\"temp_max\"), adjust from Celsius to Farenheit, do this again, and \n",
    "# assign all the math to the variable \"weather.\"\"\n",
    "\n",
    "weather.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(month=11, avg_monthly_rain=160.625)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# month with the most rain on average\n",
    "\n",
    "(weather\n",
    ".withColumn(\"month\", month(\"date\"))\n",
    ".withColumn(\"year\", year(\"date\"))\n",
    ".groupBy(\"month\", \"year\")\n",
    ".agg(sum(\"precipitation\").alias(\"total_monthly_precipitation\"))\n",
    ".groupBy(\"month\")\n",
    ".agg(mean(\"total_monthly_precipitation\").alias(\"avg_monthly_rain\"))\n",
    ".sort(col(\"avg_monthly_rain\").desc())\n",
    ".first()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(year=2012, total_winds=1244.6999999999998)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# windiest year\n",
    "\n",
    "(weather\n",
    ".withColumn(\"year\", year(\"date\"))\n",
    ".groupby(\"year\")\n",
    ".agg(sum(\"wind\").alias(\"total_winds\"))\n",
    ".sort(col(\"total_winds\").desc())\n",
    ".first()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|weather|count|\n",
      "+-------+-----+\n",
      "|    fog|   38|\n",
      "|   rain|   35|\n",
      "|    sun|   33|\n",
      "|drizzle|   10|\n",
      "|   snow|    8|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# most frequent type of weather in January\n",
    "\n",
    "(weather\n",
    ".withColumn(\"month\", month(\"date\"))\n",
    ".filter(col(\"month\") == 1)\n",
    ".groupBy(\"weather\")\n",
    ".count()\n",
    ".sort(col(\"count\").desc())\n",
    ".show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|  average_hi_temp| average_low_temp|\n",
      "+-----------------+-----------------+\n",
      "|80.29192307692308|57.52884615384615|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# avg hi and low temps on sunny days in July 2013 and 2014\n",
    "\n",
    "(weather\n",
    ".filter(month(\"date\") == 7)\n",
    ".filter(year(\"date\") > 2012)\n",
    ".filter(year(\"date\") < 2015)\n",
    ".filter(col(\"weather\") == lit(\"sun\"))\n",
    ".agg(avg(\"temp_max\").alias(\"average_hi_temp\"), avg(\"temp_min\").alias(\"average_low_temp\"))\n",
    ".show()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           avg(rain)|\n",
      "+--------------------+\n",
      "|0.021739130434782608|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# percentage of rainy days in Q3 of 2015\n",
    "\n",
    "# tip: measure a rainy day by weather == rain\n",
    "\n",
    "(weather\n",
    ".filter(year(\"date\") == 2015)\n",
    ".filter(quarter(\"date\") == 3)\n",
    ".select(when(col(\"weather\") == \"rain\", 1).otherwise(0).alias(\"rain\"))\n",
    ".agg(mean(\"rain\"))\n",
    ".show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          avg(rain)|\n",
      "+-------------------+\n",
      "|0.18478260869565216|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# % of days it rained (had non-zero precipitation) for each year:\n",
    "\n",
    "(weather\n",
    ".filter(year(\"date\") == 2015)\n",
    ".filter(quarter(\"date\") == 3)\n",
    ".select(when(col(\"precipitation\") > 0, 1).otherwise(0).alias(\"rain\"))\n",
    ".agg(mean(\"rain\"))\n",
    ".show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
